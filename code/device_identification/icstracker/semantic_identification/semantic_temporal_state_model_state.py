# -- Coding: utf-8 --
# @Version: 1.0.0
# @Time: 2024/11/23 15:32
from collections import defaultdict

import numpy as np
from numpy import mean

from device_identification.icstracker.config.config import *
from device_identification.icstracker.semantic_identification.semantic_util import compare_semantic_hashes


sigma_map = {  # a customized threshold based on the three-sigma rule: 68.27–95.45–99.73 from 1 sigma to 3 sigma
    1: 1,
    2: 0.90,
    3: 0.75,
    -1: 0
}


class SemanticTemporalStateModelState:
    def __init__(self, operation, flow_ts_packets, length_sequence, time_distributions, semantic_hashes):
        self.operation = operation
        self.flow_ts_packets = flow_ts_packets
        if flow_ts_packets:  # generated by flow_ts_packets
            self.length_sequence = self.extract_length_sequence()
            self.time_distributions = self.extract_time_distributions()
            self.semantic_hashes = self.extract_semantic_hashes()
        else:  # generated by parameters
            self.length_sequence = length_sequence
            self.time_distributions = time_distributions
            self.semantic_hashes = semantic_hashes
        self.current_index = 0  # Current position in the sequence to check against
        self.matched_packets = []
        self.matched_probabilities = []


    def extract_length_sequence(self):
        """
        Extract the length sequence from the aggregated ts_packets.

        Returns
        -------
        length_sequence: list, the length sequence
        """
        length_sequence = []
        for flow_key, flow in self.flow_ts_packets.items():
            for i in range(len(flow)):
                length_sequence.append(flow[i][2])
            break
        return length_sequence


    def extract_time_distributions(self):
        """
        Extract the mean and standard deviation of each time distributions

        Returns
        -------
        time_distributions: dictionary, the mean and standard deviation of each time distributions
        """
        time_dict = defaultdict(list)
        for key, flow in self.flow_ts_packets.items():
            for i in range(len(flow)):
                time_dict[i].append(float(flow[i][0]))  # flow[i][1]: time

        time_distributions = {}
        for key, times in time_dict.items():
            mean = np.mean(times)
            std_dev = np.std(times)
            time_distributions[key] = (mean, std_dev)
        return time_distributions


    def extract_semantic_hashes(self):
        """
        Extract the semantic hashes from packets in terms of states.

        Returns
        -------

        """
        semantic_hashes = defaultdict(list)
        for key, flow in self.flow_ts_packets.items():
            for i in range(len(flow)):
                semantic_hashes[i].append(flow[i][3])  # flow[i][3]: semantic hash
        return semantic_hashes


    def check_time_range_auto(self, my_time):
        """
        Chech whether my_time is within the time range.

        Parameters
        ----------
        my_time: the relative time of the ts_packet

        Returns
        -------
        sigma_map[my_n_sigma]: a customized match probability
        """
        my_n_sigma = -1
        for n_sigma in range(1, 4):
            lower_bound = self.time_distributions[self.current_index][0] - self.time_distributions[self.current_index][1] * n_sigma
            upper_bound = self.time_distributions[self.current_index][0] + self.time_distributions[self.current_index][1] * n_sigma
            if lower_bound <= my_time <= upper_bound:
                my_n_sigma = n_sigma
                break
        original_pro = sigma_map[my_n_sigma]
        modified_pro = original_pro * (1 - self.time_distributions[self.current_index][1] * n_sigma)
        return modified_pro


    def check_semantic_hash(self, my_hash):
        """
        Compare the hash of my_packet with all hashes of the current state.

        Parameters
        ----------
        my_hash

        Returns
        -------
        similarity score percent
        """
        similarity_scores = []
        for hash in self.semantic_hashes[self.current_index]:
            similarity_scores.append(compare_semantic_hashes(hash, my_hash))
        return mean(similarity_scores)


    def match_auto_semantic(self, ts_packet):
        """
        Match the ts_packet with the current TSM by the directional length, the relative time, and semantic hash

        Parameters
        ----------
        ts_packet

        Returns
        -------
        my_score: match probability
        """
        if ts_packet[2] == self.length_sequence[self.current_index]:  # if the packet length is unmatched, continue to the next packet
            ratio = GLOBAL_DL_RATIO  # at least half of packets satisfy 3*sigma (0.75), and all packets match all directional sequences
            time_score = self.check_time_range_auto(ts_packet[0])
            sequence_score = 1
            my_score = sequence_score * ratio + time_score * (1-ratio)
            semantic_score = self.check_semantic_hash(ts_packet[3])
            self.current_index += 1
            self.matched_packets.append(ts_packet)
            self.matched_probabilities.append((my_score, semantic_score))
            return 1
        else:
            return -1


    def check_auto(self, flow_semantic_packets):
        """
        Check whether a sequence of ts_packets match with the STSM

        Parameters
        ----------
        flow_ts_packets: a sequence of ts_packets

        Returns
        -------
        match probability
        """
        for pkt in flow_semantic_packets:
            if self.match_auto_semantic(pkt) > 0:
                if self.is_totally_matched():  # the size of flow_semantic_packets may be larger than that of this model
                    break

        if len(self.matched_probabilities) == 0:
            return 0, 0
        # incomplete match
        time_scores = [item[0] for item in self.matched_probabilities]
        semantic_scores = [item[1] for item in self.matched_probabilities]
        packet_match_ratio = len(self.matched_packets) / len(self.length_sequence)
        time_score_uni = mean(time_scores) * len(self.matched_packets) / len(flow_semantic_packets)
        semantic_score_uni = mean(semantic_scores) * len(self.matched_packets) / len(flow_semantic_packets)
        # match_score_bias = 0.25
        # packet_match_ratio = len(self.matched_packets) / max(len(flow_semantic_packets), len(self.length_sequence))
        # time_score_uni = mean(time_scores) * packet_match_ratio
        # semantic_score_uni = mean(semantic_scores) * packet_match_ratio
        # if model_match_ratio < 1 or time_score_uni < 0.6:  # at least half of them matched.
        #     return 0, 0
        if packet_match_ratio < GLOBAL_PACKET_MATCH:  # at least half of them matched.
            return 0, 0
        return time_score_uni, semantic_score_uni


    def stsm_state_check(self, flow_semantic_packets):
        """
        The main interface of SemanticTemporalStateModelState for checking input data

        Parameters
        ----------
        flow_semantic_packets

        Returns
        -------

        """
        return self.check_auto(flow_semantic_packets)


    def is_totally_matched(self):
        if self.current_index == len(self.length_sequence):
            return True


    def reset(self):
        """
        Reset the state of the temporal state model

        Returns
        -------
        None
        """
        self.current_index = 0  # Current position in the sequence to check against
        self.matched_packets = []
        self.matched_probabilities = []


    def to_string_semantic_hashes(self):
        tuples = []
        for i in range(len(self.length_sequence)):
            hashes = "&&".join(self.semantic_hashes[i])
            tuples.append((self.length_sequence[i], f"{self.time_distributions[i][0]:.6f}", f"{self.time_distributions[i][1]:.6f}", hashes))
        # Convert tuples to a string
        result_string = ", ".join(f"({tuple[0]}::{tuple[1]}::{tuple[2]}::{tuple[3]})" for tuple in tuples)
        return result_string


    @classmethod
    def construct_flows(cls, operation, flow_ts_packets):
        """
        Construct a STSM object from flow_ts_packets.

        Parameters
        ----------
        operation: operation string
        flow_ts_packets: a flow of ts_packets

        Returns
        -------
        STSM object
        """
        return cls(operation, flow_ts_packets, length_sequence = None, time_distributions = None, semantic_hashes= None)


    @classmethod
    def construct_str_stsm_semantic_hashes(cls, operation, str_stsm):
        """
        Construct a STSM object from a STSM string.

        Parameters
        ----------
        operation: operation string
        str_tsm: STSM string

        Returns
        -------
        STSM object
        """
        flow_ts_packets = None

        str_states = str_stsm.split(", ")
        length_sequence = []
        time_distributions = {}
        semantic_hashes = {}
        for i in range(len(str_states)):
            elements = str_states[i].strip("()").split("::")
            length_sequence.append(elements[0])
            time_distributions[i] = (float(elements[1]), float(elements[2]))
            semantic_hashes[i] = elements[3].split("&&")
        return cls(operation, flow_ts_packets, length_sequence, time_distributions, semantic_hashes)